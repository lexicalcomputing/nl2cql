{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbaab85e-cefb-46a8-bf9e-0a7810ffc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import json\n",
    "from cqlcmp import cqlcmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0b86bf-0963-4944-87c5-51add03cac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(21)\n",
    "torch.cuda.manual_seed_all(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c264eac-c63a-4b2b-bba6-ec98b5cf4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395ed951-d6c5-4e7c-8fd7-907f520e0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQL(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: Optional[str] = None) -> None:\n",
    "        self.sentence_freq = []\n",
    "        self.cql2nl = []\n",
    "        self.nl2cql = []\n",
    "        self.natural_language_rulebased = []\n",
    "        self.cql = []\n",
    "        self.natural_language = []\n",
    "\n",
    "        if path is not None:\n",
    "            self.load_tsv(path)\n",
    "\n",
    "    def add_translation(self, freq: int, cql: str, natural_language_rulebased: str, natural_language: List[str]) -> None:\n",
    "        cql_index = len(self.sentence_freq)\n",
    "        self.sentence_freq.append(freq)\n",
    "        self.cql.append(cql)\n",
    "        self.natural_language_rulebased.append(natural_language_rulebased)\n",
    "        self.cql2nl.append([])\n",
    "\n",
    "        for sentence in natural_language:\n",
    "            self.nl2cql.append(cql_index)\n",
    "            self.cql2nl[-1].append(len(self.natural_language))\n",
    "            self.natural_language.append(sentence)\n",
    "\n",
    "    def load_tsv(self, path: str) -> None:\n",
    "        with open(path, \"r\") as file_data:\n",
    "            for line in file_data:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                texts_json = json.loads(line[4])\n",
    "                texts_extracted = texts_json[\"data\"][0][\"content\"][0][\"text\"][\"value\"].split(\"\\n\")\n",
    "                self.add_translation(int(line[0]), line[2], line[3], texts_extracted)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nl2cql)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.nl2cql):\n",
    "            return self.natural_language[idx], self.cql[self.nl2cql[idx]]\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac3fcf7-1e5f-4623-a448-0649a87b5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQLTokenized(DatasetNatural2CQL):\n",
    "    def __init__(self, tokenizer: Any, path: Optional[str] = None) -> None:\n",
    "        super().__init__(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.natural_language_tokenized = []\n",
    "        self.natural_language_mask = []\n",
    "        self.cql_tokenized = []\n",
    "        if len(self) > 0:\n",
    "            self.tokenize()\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        for sentence in self.natural_language:\n",
    "            sentence_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [\"translate: \" + sentence.replace(\"/\", \"//\")],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.natural_language_tokenized.append(sentence_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "            self.natural_language_mask.append(sentence_tokenized.attention_mask.squeeze().to(dtype=torch.long))\n",
    "\n",
    "        for c in self.cql:\n",
    "            c_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [c],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.cql_tokenized.append(c_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "\n",
    "    def apply_padding(self, mx0, mx2) -> None:\n",
    "        for i in range(len(self.natural_language_tokenized)):\n",
    "            tmp = self.natural_language_mask[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_mask[i] = torch.zeros(mx0)\n",
    "            self.natural_language_mask[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_mask[i] = self.natural_language_mask[i].to(dtype=torch.long)\n",
    "\n",
    "            tmp = self.natural_language_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_tokenized[i] = torch.zeros(mx0)\n",
    "            self.natural_language_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_tokenized[i] = self.natural_language_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "        for i in range(len(self.cql_tokenized)):\n",
    "            tmp = self.cql_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.cql_tokenized[i] = torch.zeros(mx2)\n",
    "            self.cql_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.cql_tokenized[i] = self.cql_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.nl2cql):\n",
    "            return self.natural_language_tokenized[idx], self.natural_language_mask[idx], self.cql_tokenized[self.nl2cql[idx]]\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966413af-2754-450c-80c7-9302b46004da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQLTokenizedSplited(DatasetNatural2CQL):\n",
    "    def __init__(self, tokenizer: Any, path: Optional[str] = None) -> None:\n",
    "        super().__init__(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.natural_language_tokenized = []\n",
    "        self.natural_language_mask = []\n",
    "        self.cql_tokenized = []\n",
    "        self.enabled_natural_language = []\n",
    "        if path is not None:\n",
    "            self.tokenize()\n",
    "\n",
    "    def split_on_cql(self, p):\n",
    "        s = self.cql2nl[:]\n",
    "        random.shuffle(s)\n",
    "        sp = int(len(s) * p / 100)\n",
    "        return s[:sp], s[sp:]\n",
    "    \n",
    "    def enable_cql(self, cqls):\n",
    "        self.enabled_natural_language = []\n",
    "        for cql in cqls:\n",
    "            for p in cql:\n",
    "                self.enabled_natural_language.append(p)\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        for sentence in self.natural_language:\n",
    "            sentence_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [\"translate: \" + sentence.replace(\"/\", \"//\")],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.natural_language_tokenized.append(sentence_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "            self.natural_language_mask.append(sentence_tokenized.attention_mask.squeeze().to(dtype=torch.long))\n",
    "\n",
    "        for c in self.cql:\n",
    "            c_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [c],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.cql_tokenized.append(c_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "\n",
    "    def apply_padding(self, mx0, mx2) -> None:\n",
    "        for i in range(len(self.natural_language_tokenized)):\n",
    "            tmp = self.natural_language_mask[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_mask[i] = torch.zeros(mx0)\n",
    "            self.natural_language_mask[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_mask[i] = self.natural_language_mask[i].to(dtype=torch.long)\n",
    "\n",
    "            tmp = self.natural_language_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_tokenized[i] = torch.zeros(mx0)\n",
    "            self.natural_language_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_tokenized[i] = self.natural_language_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "        for i in range(len(self.cql_tokenized)):\n",
    "            tmp = self.cql_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.cql_tokenized[i] = torch.zeros(mx2)\n",
    "            self.cql_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.cql_tokenized[i] = self.cql_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enabled_natural_language)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.enabled_natural_language):\n",
    "            return self.natural_language_tokenized[self.enabled_natural_language[idx]], self.natural_language_mask[self.enabled_natural_language[idx]], self.cql_tokenized[self.nl2cql[self.enabled_natural_language[idx]]]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3598ddd-dd1f-453c-b446-61508c5e1d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c77a68-3ed0-41ce-afb2-f386a9e87e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/asteria02/thor1/asteria04_mnt_local2/xmikusek/aiproject/.venv/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = DatasetNatural2CQLTokenizedSplited(tokenizer, \"expand_natural_texts_0004.res.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a0dec3-e66a-4321-96e5-62e696e8243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cqls, other_cqls = dataset_tokenized.split_on_cql(80)\n",
    "sp = int(len(other_cqls) * 80 / 100)\n",
    "valid_cqls, test_cqls = other_cqls[:sp], other_cqls[sp:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f64bb8f-5161-43bf-a857-deee2d1caeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized.enable_cql(test_cqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21665ab7-73a0-464d-bb2f-a30a7c4d7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([13959,    10,    71,   712, 14145,    28,     8,  1448,  6047,     5,\n",
       "             1]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor([  784,  6051, 17592, 19778,     7,   121,   908,     1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset_tokenized))\n",
    "dataset_tokenized[len(dataset_tokenized)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95149b18-ad91-4d19-9eb5-93b07698a2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/google-t5_t5-base/model_train_data_34_0.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d65edb8d-89a2-4959-91e1-0c77cda72b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>][lemma=\"door\" & tag=\"N.*\"][word=\"open\" & tag=\"N.*\"] within <unk>s/>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "sentence_tokenized = tokenizer(\n",
    "    \"translate: Any word followed by lemma door as tag noun and then followed by a word starting with open within sentence.\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "generated_ids = model.generate(\n",
    "      sentence_tokenized.input_ids.to(\"cuda\"),\n",
    "        max_new_tokens=256,\n",
    ")\n",
    "result = \"\".join(tokenizer.convert_ids_to_tokens(generated_ids[0])[:-1])\n",
    "result = result.replace(\"▁\", \" \")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fe22a8d-8588-4cec-9103-de4310c9d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[word=\"a.l<unk>1,3<unk>\"]\n",
      "[word=\"a.?l<unk>1,3<unk>\"]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/asteria02/thor1/asteria04_mnt_local2/xmikusek/aiproject/.venv/lib/python3.10/site-packages/requests/models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(cql_gold)\n\u001b[0;32m---> 55\u001b[0m cmp_result \u001b[38;5;241m=\u001b[39m \u001b[43mcqlcmp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcql_gold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreloaded/bnc2_tt31\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSKETCH_ENGINE_API_KEY\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m912b3694e685d7ff2b4cc8fdfe0e94cd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m dic_add(average_sum, cmp_result)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset_tokenized)), file\u001b[38;5;241m=\u001b[39melog)\n",
      "File \u001b[0;32m/mnt/asteria02/thor1/asteria04_mnt_local2/xmikusek/aiproject_google-t5_t5-base/cqlcmp.py:141\u001b[0m, in \u001b[0;36mcqlcmp\u001b[0;34m(gold_cql, cql, corpname, limit, SKETCH_ENGINE_API_KEY, verbose)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCQL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgold_cql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 141\u001b[0m response_gold_json \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://app.sketchengine.eu/bonito/run.cgi/concordance?corpname=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquote_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m&default_attr=word&attrs=pos&attr_allpos=all&viewmode=sen&cup_hl=q&structs=s,g&fromp=1&pagesize=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m&kwicleftctx=0&kwicrightctx=0&json=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mjson_gold_param\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEADERS\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response_gold_json:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ON_ERROR\n",
      "File \u001b[0;32m/mnt/asteria02/thor1/asteria04_mnt_local2/xmikusek/aiproject/.venv/lib/python3.10/site-packages/requests/models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def average(av_sum, d):\n",
    "    res = {}\n",
    "    for k in av_sum:\n",
    "        res[k] = av_sum[k] / d\n",
    "    return res\n",
    "\n",
    "def dic_add(av_sum, to_add):\n",
    "    for k in av_sum:\n",
    "        av_sum[k] += to_add[k]\n",
    "\n",
    "average_sum = {\n",
    "    \"n_grams_1\": 0,\n",
    "    \"n_grams_2\": 0,\n",
    "    \"n_grams_3\": 0,\n",
    "    \"n_grams_4\": 0,\n",
    "    \"n_grams_5\": 0,\n",
    "    \"n_grams_6\": 0,\n",
    "    \"bleu\": 0,\n",
    "    \"precision\": 0,\n",
    "    \"recall\": 0,\n",
    "    \"f1\": 0,\n",
    "    \"maximal intersection over union\": 0,\n",
    "    \"maximal intersection over union +-1\": 0,\n",
    "    \"maximal intersection over union +-2\": 0,\n",
    "    \"sentence_precision\": 0,\n",
    "    \"sentence_recall\": 0,\n",
    "}\n",
    "\n",
    "with open(\"eval_log\", \"w\") as elog:\n",
    "    for i, gold in enumerate(dataset_tokenized):\n",
    "        cql_gold = \"\".join(tokenizer.convert_ids_to_tokens(gold[2])[:-1])\n",
    "        cql_gold = cql_gold.replace(\"▁\", \" \")\n",
    "        cql_gold = cql_gold.strip()\n",
    "        generated_ids = model.generate(\n",
    "            gold[0].unsqueeze(0).to(\"cuda\"),\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        result = \"\".join(tokenizer.convert_ids_to_tokens(generated_ids[0])[:-1])\n",
    "        result = result.replace(\"▁\", \" \")\n",
    "        result = result.replace(\"<pad>\", \" \")\n",
    "        result = result.strip()\n",
    "    \n",
    "        lb = result.find(\"[\")\n",
    "        rb = result.find(\"]\")\n",
    "        if rb != -1:\n",
    "            if lb == -1 or lb > rb:\n",
    "                result = \"[\" + result\n",
    "        print(result)\n",
    "        print(cql_gold)\n",
    "    \n",
    "        cmp_result = cqlcmp(cql_gold, result, \"preloaded/bnc2_tt31\", SKETCH_ENGINE_API_KEY = \"912b3694e685d7ff2b4cc8fdfe0e94cd\")\n",
    "        dic_add(average_sum, cmp_result)\n",
    "        print(str(i+1) + \"/\" + str(len(dataset_tokenized)), file=elog)\n",
    "        print(average(average_sum, i+1), file=elog)\n",
    "        elog.flush()\n",
    "        if i + 1 == len(dataset_tokenized):\n",
    "            break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46361d26-34ca-44fc-93ef-81f3651ba730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
