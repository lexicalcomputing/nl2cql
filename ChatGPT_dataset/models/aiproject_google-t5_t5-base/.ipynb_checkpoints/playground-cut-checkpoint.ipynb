{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbaab85e-cefb-46a8-bf9e-0a7810ffc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b86bf-0963-4944-87c5-51add03cac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(21)\n",
    "torch.cuda.manual_seed_all(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c264eac-c63a-4b2b-bba6-ec98b5cf4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ed951-d6c5-4e7c-8fd7-907f520e0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQL(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: Optional[str] = None) -> None:\n",
    "        self.sentence_freq = []\n",
    "        self.cql2nl = []\n",
    "        self.nl2cql = []\n",
    "        self.natural_language_rulebased = []\n",
    "        self.cql = []\n",
    "        self.natural_language = []\n",
    "\n",
    "        if path is not None:\n",
    "            self.load_tsv(path)\n",
    "\n",
    "    def add_translation(self, freq: int, cql: str, natural_language_rulebased: str, natural_language: List[str]) -> None:\n",
    "        cql_index = len(self.sentence_freq)\n",
    "        self.sentence_freq.append(freq)\n",
    "        self.cql.append(cql)\n",
    "        self.natural_language_rulebased.append(natural_language_rulebased)\n",
    "        self.cql2nl.append([])\n",
    "\n",
    "        for sentence in natural_language:\n",
    "            self.nl2cql.append(cql_index)\n",
    "            self.cql2nl[-1].append(len(self.natural_language))\n",
    "            self.natural_language.append(sentence)\n",
    "\n",
    "    def load_tsv(self, path: str) -> None:\n",
    "        with open(path, \"r\") as file_data:\n",
    "            for line in file_data:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                texts_json = json.loads(line[4])\n",
    "                texts_extracted = texts_json[\"data\"][0][\"content\"][0][\"text\"][\"value\"].split(\"\\n\")\n",
    "                self.add_translation(int(line[0]), line[2], line[3], texts_extracted)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nl2cql)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.nl2cql):\n",
    "            return self.natural_language[idx], self.cql[self.nl2cql[idx]]\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3fcf7-1e5f-4623-a448-0649a87b5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQLTokenized(DatasetNatural2CQL):\n",
    "    def __init__(self, tokenizer: Any, path: Optional[str] = None) -> None:\n",
    "        super().__init__(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.natural_language_tokenized = []\n",
    "        self.natural_language_mask = []\n",
    "        self.cql_tokenized = []\n",
    "        if len(self) > 0:\n",
    "            self.tokenize()\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        for sentence in self.natural_language:\n",
    "            sentence_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [\"translate: \" + sentence.replace(\"/\", \"//\")],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.natural_language_tokenized.append(sentence_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "            self.natural_language_mask.append(sentence_tokenized.attention_mask.squeeze().to(dtype=torch.long))\n",
    "\n",
    "        for c in self.cql:\n",
    "            c_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [c],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.cql_tokenized.append(c_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "\n",
    "    def apply_padding(self, mx0, mx2) -> None:\n",
    "        for i in range(len(self.natural_language_tokenized)):\n",
    "            tmp = self.natural_language_mask[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_mask[i] = torch.zeros(mx0)\n",
    "            self.natural_language_mask[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_mask[i] = self.natural_language_mask[i].to(dtype=torch.long)\n",
    "\n",
    "            tmp = self.natural_language_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_tokenized[i] = torch.zeros(mx0)\n",
    "            self.natural_language_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_tokenized[i] = self.natural_language_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "        for i in range(len(self.cql_tokenized)):\n",
    "            tmp = self.cql_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.cql_tokenized[i] = torch.zeros(mx2)\n",
    "            self.cql_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.cql_tokenized[i] = self.cql_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "    def cut_data(self, mx0, mx2):\n",
    "        for i in range(len(self.natural_language_tokenized)):\n",
    "            self.natural_language_mask[i] = self.natural_language_tokenized[i][:mx0]\n",
    "            self.natural_language_mask[i] = self.natural_language_mask[i][:mx0]\n",
    "\n",
    "        for i in range(len(self.cql_tokenized)):\n",
    "            self.cql_tokenized[i] = self.cql_tokenized[i][:mx2]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.nl2cql):\n",
    "            return self.natural_language_tokenized[idx], self.natural_language_mask[idx], self.cql_tokenized[self.nl2cql[idx]]\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3598ddd-dd1f-453c-b446-61508c5e1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afaf97-672c-4358-af98-2f6b57b7d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetNatural2CQL(\"expand_natural_texts_0004.res.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c77a68-3ed0-41ce-afb2-f386a9e87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized = DatasetNatural2CQLTokenized(tokenizer, \"expand_natural_texts_0004.res.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edbd8d-a260-407a-8e20-f9aecfc241ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized[106102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f036b7-d9f6-4d91-ba44-cbf768f5204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx0 = 0\n",
    "mx2 = 0\n",
    "for i, rec in enumerate(dataset_tokenized):\n",
    "    if i >= len(dataset_tokenized):\n",
    "        break\n",
    "    if len(rec[0].shape) > 0 and len(rec[0]) > mx0:\n",
    "        mx0 = rec[0].shape[0]\n",
    "    if len(rec[2].shape) > 0 and len(rec[2]) > mx2:\n",
    "        mx2 = rec[2].shape[0]\n",
    "print(\"mx0 =\", mx0, \"mx2 =\", mx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890c48c-eb44-4674-86f3-9ddc4aec5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized.apply_padding(mx0, mx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936ff3a-09e0-468c-9227-ae9392795545",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized[106102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78025644-9b22-4069-9bcb-3a6412d1f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized.cut_data(120, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e90829-82da-4583-a370-2696ed19b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized[106102][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2627e-75ef-4cb4-9ed0-95229483f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired from: https://github.com/Shivanandroy/T5-Finetuning-PyTorch\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    epoch_start = datetime.datetime.utcnow()\n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[2].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[0].to(device, dtype=torch.long)\n",
    "        mask = data[1].to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "\n",
    "        time_delta = datetime.datetime.utcnow() - epoch_start\n",
    "        if _ % 100 == 0:\n",
    "            print(\"time: \", datetime.datetime.utcnow().isoformat(), time_delta.seconds , \"sec | epoch: \", str(epoch), \"| batch: \", str(_), \"/\", len(loader), \"|\", str(loss.item()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7f91-4da5-4bdc-9f12-4fd1380b20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307bb6a-22c2-4150-b026-271e608ab601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(0, tokenizer, model, device, torch.utils.data.DataLoader(dataset_tokenized, batch_size=4, shuffle=True, num_workers=0), optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65edb8d-89a2-4959-91e1-0c77cda72b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sentence_tokenized = tokenizer(\n",
    "    \"translate: Aby word police.\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(sentence_tokenized)\n",
    "generated_ids = model.generate(\n",
    "      sentence_tokenized.input_ids.to(\"cuda\")\n",
    ")\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e82a70-d44b-4737-8768-c1eccbe462df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([ 0,  891,   63, 1448, 2095,    1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
