{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf874df-65bb-4853-9769-c2a56dec9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import random\n",
    "from math import inf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import datasets\n",
    "import time\n",
    "\n",
    "import json\n",
    "import os\n",
    "import cqlcmp\n",
    "import cql_checker\n",
    "import lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd5b14-0616-4d80-9536-d6e8430e946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(21)\n",
    "torch.cuda.manual_seed_all(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3527179-090d-48ae-bdb5-f4eef462cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc981f2-f58d-4be8-88f6-955d973455cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQL(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: Optional[str] = None) -> None:\n",
    "        self.sentence_freq = []\n",
    "        self.cql2nl = []\n",
    "        self.nl2cql = []\n",
    "        self.natural_language_rulebased = []\n",
    "        self.cql = []\n",
    "        self.natural_language = []\n",
    "        self.enabled_natural_language = []\n",
    "\n",
    "        if path is not None:\n",
    "            self.load_tsv(path)\n",
    "\n",
    "    def enable_cql(self, cqls):\n",
    "        self.enabled_natural_language = []\n",
    "        for cql in cqls:\n",
    "            for p in cql:\n",
    "                self.enabled_natural_language.append(p)\n",
    "\n",
    "    def dump_json(self, filepath: str) -> None:\n",
    "        with open(filepath, \"w\") as file:\n",
    "            for i in range(len(self)):\n",
    "                data = json.dumps(self[i])\n",
    "                file.write(data)\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "    def add_translation(self, freq: int, cql: str, natural_language_rulebased: str, natural_language: List[str]) -> None:\n",
    "        cql_index = len(self.sentence_freq)\n",
    "        self.sentence_freq.append(freq)\n",
    "        self.cql.append(cql)\n",
    "        self.natural_language_rulebased.append(natural_language_rulebased)\n",
    "        self.cql2nl.append([])\n",
    "\n",
    "        for sentence in natural_language:\n",
    "            self.nl2cql.append(cql_index)\n",
    "            self.cql2nl[-1].append(len(self.natural_language))\n",
    "            self.natural_language.append(sentence)\n",
    "\n",
    "    def load_tsv(self, path: str) -> None:\n",
    "        with open(path, \"r\") as file_data:\n",
    "            for line in file_data:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                texts_json = json.loads(line[4])\n",
    "                texts_extracted = texts_json[\"data\"][0][\"content\"][0][\"text\"][\"value\"].split(\"\\n\")\n",
    "                self.add_translation(int(line[0]), line[2], line[3], texts_extracted)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enabled_natural_language)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.nl2cql):\n",
    "            return {\"text\": self.natural_language[self.enabled_natural_language[idx]], \"cql\": self.cql[self.nl2cql[self.enabled_natural_language[idx]]]}\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02dab6-961b-48c7-bf75-ef953c5c9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetNatural2CQL(\"expand_natural_texts_0004.res.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbff40f-252f-483e-8572-225ee838bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cqls = []\n",
    "with open(\"test_ids.json\", \"r\") as file:\n",
    "    validation_cqls = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0898f-9db3-4673-b22c-9de62a79e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.enable_cql(validation_cqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19722e4e-b833-47f5-b075-0c57d388c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df87f9-d445-49a8-bf6b-52f1be2da4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google-t5/t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42c57a-b4aa-49bf-9990-e06d872b1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'models/google-t5_t5-base'\n",
    "\n",
    "# List all folders (directories) in the given path\n",
    "folders = [name for name in os.listdir(path) if not os.path.isdir(os.path.join(path, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f849b-cfe3-40f6-848a-809077ba6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_in(data):\n",
    "    full_input = \"translate: \" + data\n",
    "    return full_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab3133-91ec-4dd6-a384-fbc438460706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.shuffle(dataset.enabled_natural_language)\n",
    "\n",
    "i = 0\n",
    "while i < len(dataset):\n",
    "    dataset.natural_language[dataset.enabled_natural_language[i]] = f_in(dataset.natural_language[dataset.enabled_natural_language[i]])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1ec7a-0966-4d02-ae42-33516b14bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(text):\n",
    "    text = text.split(\"translate: \", 1)[1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527539df-d36d-4ed5-bf7d-7e6f74291562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQLLogitsProcessor(transformers.generation.logits_process.LogitsProcessor):\n",
    "    def __init__(self, id2string, verbose = False):\n",
    "        self.id2string = id2string\n",
    "        self.checker = cql_checker.CQLChecker()\n",
    "        self.checker.eos_token_id = 1\n",
    "        self.verbose = verbose\n",
    "        self.text = \"\"\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        scores[0][2] = -inf\n",
    "        scores[0][3] = -inf\n",
    "        if len(self.text) == 0:\n",
    "            scores[0][784] = inf\n",
    "            self.text = \"[\"\n",
    "            return scores\n",
    "        while True:\n",
    "            token_id = int(np.argmax(scores[0].to(\"cpu\")))\n",
    "            if token_id < 32000:\n",
    "                text_copy = self.text\n",
    "                text_copy += self.id2string[token_id]\n",
    "                is_error = False\n",
    "                try:\n",
    "                    self.checker.parse(text_copy)\n",
    "                except lark.exceptions.UnexpectedEOF:\n",
    "                    if token_id == self.checker.eos_token_id:\n",
    "                        is_error = True\n",
    "                except Exception:\n",
    "                    is_error = True\n",
    "                if not is_error:\n",
    "                    scores[0][token_id] = inf\n",
    "                    self.text = text_copy\n",
    "                    if self.verbose:\n",
    "                        print(\"Accepted: \", self.id2string[token_id])\n",
    "                    break\n",
    "            scores[0][token_id] = -inf\n",
    "            if self.verbose:\n",
    "                print(\"Rejected: \", self.id2string[token_id])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e50fe-6137-47c2-88f6-0b5e13b066b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = tokenizer.convert_ids_to_tokens(list(range(tokenizer.vocab_size)))\n",
    "id2string = []\n",
    "for token in id2token:\n",
    "    id2string.append(tokenizer.convert_tokens_to_string([token]))\n",
    "processor = CQLLogitsProcessor(id2string, False)\n",
    "logits_processor_list = transformers.generation.logits_process.LogitsProcessorList([\n",
    "    processor,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea26fc-d306-49ab-92df-d2ef783fc5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_results_logits.tsv\", \"w\") as test_tsv:\n",
    "    with open(\"test_log.txt_logits\", \"w\") as log:\n",
    "        for folder in folders:\n",
    "            start_time = int(time.time())\n",
    "            adapter_model_name = 'models/google-t5_t5-base/' + folder\n",
    "\n",
    "            if folder != \"model_train_data_20_final.pt\":\n",
    "                continue\n",
    "            \n",
    "            model.load_state_dict(torch.load(adapter_model_name, weights_only=True))\n",
    "            model.eval()\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "            blue_sum = 0\n",
    "            blue_size = 0\n",
    "            batch_id = 0\n",
    "            for batch in dataloader:\n",
    "                batch_id += 1\n",
    "                inputs = tokenizer(batch[\"text\"], truncation=True, max_length=1024, return_tensors=\"pt\", padding=True).to(device)\n",
    "                outputs = model.generate(**inputs, max_new_tokens=100, logits_processor=logits_processor_list)\n",
    "                processor.text = \"\"\n",
    "                for i, output in enumerate(outputs):\n",
    "                    cql = tokenizer.decode(output, skip_special_tokens=True).replace(\"<extra_id_0>\", \"\")\n",
    "                    cql_ids = cqlcmp.cql_tokenizer(cql)\n",
    "                    gold_cql_ids = cqlcmp.cql_tokenizer(batch[\"cql\"][i])\n",
    "                    bleu = cqlcmp.sentence_bleu([gold_cql_ids], cql_ids, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "                    blue_sum += bleu\n",
    "                    blue_size += 1\n",
    "                    test_result = [cql.replace(\"\\n\", \" \"), batch[\"cql\"][i], get_text(batch[\"text\"][i])]\n",
    "                    print(\"\\t\".join(test_result))\n",
    "                    print(\"\\t\".join(test_result), file=test_tsv)\n",
    "                to_log = \"\"\n",
    "                to_log += \"Working on: \" + adapter_model_name + \" | \"\n",
    "                to_log += str(blue_size) + \"/\" + str(len(dataset)) + \" | \"\n",
    "                to_log += \"AVG Bleu: \" + str(blue_sum/blue_size) + \" | \"\n",
    "                to_log += \"Time: \" + str(int(time.time())-start_time) + \" | \"\n",
    "                try:\n",
    "                    to_log += \"ETA: \" + str(int((int(time.time())-start_time)/(blue_size/len(dataset)*(1-(blue_size/len(dataset)))))) + \" sec\" + \" | \"\n",
    "                except:\n",
    "                    to_log += \"ETA: \" + \"0\" + \" sec\" + \" | \"\n",
    "                \n",
    "                print(to_log, file=log)\n",
    "                log.flush()\n",
    "                test_tsv.flush()\n",
    "                print(to_log)\n",
    "                break\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a46976-7108-4f03-90c9-aa1312fa7dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
