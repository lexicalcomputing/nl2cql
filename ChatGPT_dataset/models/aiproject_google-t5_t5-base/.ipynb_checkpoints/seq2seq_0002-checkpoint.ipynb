{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338d427c-7041-4471-aa73-d3eadac6dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, Dataset\n",
    "\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd66115-a352-40de-ad72-65daf39a7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "torch.manual_seed(21)\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e4a75e-5f8e-46ff-b93a-52a2302f52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pandas = pd.read_csv(\"expand_natural_texts_0004.res.tsv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f81827b7-7483-4398-89f9-ed0d7df11809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pandas = dataset_pandas.sample(frac = 1)\n",
    "msk = np.random.rand(len(dataset_pandas)) < 0.9\n",
    "dataset_pandas_test = dataset_pandas[~msk]\n",
    "dataset_pandas_tmp = dataset_pandas[msk]\n",
    "\n",
    "msk = np.random.rand(len(dataset_pandas_tmp)) < 0.9\n",
    "dataset_pandas_validate = dataset_pandas_tmp[~msk]\n",
    "dataset_pandas_train = dataset_pandas_tmp[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b71c878-e208-45c6-bbf2-8012ffdb43c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/xmikusek/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a35881-d83a-4277-b7ae-66766f804792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "385d6e28-3431-41b2-9316-54b09fdec8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "EMP_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1bc60b-74e0-4822-af1b-7834ee1baf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQL(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data_unprocessed = []\n",
    "        self.natual_text = []\n",
    "        self.word2id = {\".SOS.\": SOS_token, \".EOS.\": EOS_token, \".EMP.\": EMP_token}\n",
    "        self.cql2id = {\".SOS.\": SOS_token, \".EOS.\": EOS_token, \".EMP.\": EMP_token}\n",
    "        self.texts = []\n",
    "        self.cqls = []\n",
    "\n",
    "    def load_pandas(self, dataset):\n",
    "        for line in dataset.iloc:\n",
    "            texts_json = json.loads(line[4])\n",
    "            texts_extracted = texts_json[\"data\"][0][\"content\"][0][\"text\"][\"value\"].split(\"\\n\")\n",
    "            texts_tokenized, cql_tokenized = self.make_transaltion(texts_extracted, line[2])\n",
    "            self.add_transaltions_tokenized(texts_tokenized, cql_tokenized)\n",
    "\n",
    "    def load_tsv(self, path):\n",
    "        with open(path, \"r\") as file_data:\n",
    "            for line in file_data:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                texts_json = json.loads(line[4])\n",
    "                texts_extracted = texts_json[\"data\"][0][\"content\"][0][\"text\"][\"value\"].split(\"\\n\")\n",
    "                texts_tokenized, cql_tokenized = self.make_transaltion(texts_extracted, line[2])\n",
    "                self.add_transaltions_tokenized(texts_tokenized, cql_tokenized)\n",
    "\n",
    "    def process_text(self, text):\n",
    "        self.is_in_quote = False\n",
    "        def make_triplet(token):\n",
    "            for i in range(len(token) // 3):\n",
    "                yield token[i*3:i*3+3]\n",
    "            if len(token) % 3 != 0:\n",
    "                yield token[(len(token) // 3)*3:]\n",
    "        def resolve_quotes(tokens):\n",
    "            for token in tokens:\n",
    "                if token == '``':\n",
    "                    self.is_in_quote = True\n",
    "                    yield token\n",
    "                    continue\n",
    "                if token == \"''\":\n",
    "                    self.is_in_quote = False\n",
    "                    yield token\n",
    "                    continue\n",
    "                if self.is_in_quote:\n",
    "                    for c in token:\n",
    "                        yield c\n",
    "                else:\n",
    "                    yield token\n",
    "        triplets = []\n",
    "        for word_token in resolve_quotes(word_tokenize(text)):\n",
    "            triplets = triplets + list(make_triplet(word_token))\n",
    "        return triplets\n",
    "\n",
    "    def process_cql(self, cql):\n",
    "        return cql.strip()\n",
    "\n",
    "    def make_transaltion(self, texts, cql):\n",
    "        cql_tokenized = self.process_cql(cql)\n",
    "        texts_tokenized = []\n",
    "        for text in texts:\n",
    "            texts_tokenized.append(self.process_text(text))\n",
    "        return texts_tokenized, cql_tokenized\n",
    "\n",
    "    def add_transaltions_tokenized(self, texts, cql):\n",
    "        for text in texts:\n",
    "            for t in text:\n",
    "                if t not in self.word2id:\n",
    "                    self.word2id[t] = len(self.word2id)\n",
    "        for t in cql:\n",
    "            if t not in self.cql2id:\n",
    "                self.cql2id[t] = len(self.cql2id)\n",
    "        cql_numeric = []\n",
    "        for t in cql:\n",
    "            cql_numeric.append(self.cql2id[t])\n",
    "\n",
    "        texts_numeric = []\n",
    "        for text in texts:\n",
    "            text_numeric = []\n",
    "            for t in text:\n",
    "                text_numeric.append(self.word2id[t])\n",
    "            if len(text_numeric) == 0 or len(cql_numeric) == 0:\n",
    "                continue\n",
    "            texts_numeric.append(text_numeric)\n",
    "        if len(texts_numeric) == 0:\n",
    "            return\n",
    "        self.data_unprocessed.append((texts_numeric, cql_numeric))\n",
    "                \n",
    "    def finalize(self, device):\n",
    "        self.id2cql = {}\n",
    "        self.NUMER_OF_RECORDS = 0\n",
    "        self.MAXIMAL_LENGTH = -1\n",
    "        for texts, cql in self.data_unprocessed:\n",
    "            if self.MAXIMAL_LENGTH < len(cql):\n",
    "                self.MAXIMAL_LENGTH = len(cql)\n",
    "            self.NUMER_OF_RECORDS += len(texts)\n",
    "            for text in texts:\n",
    "                if self.MAXIMAL_LENGTH < len(text):\n",
    "                    self.MAXIMAL_LENGTH = len(text)\n",
    "\n",
    "        in_values = np.zeros((self.NUMER_OF_RECORDS, self.MAXIMAL_LENGTH+1), dtype=np.int32)\n",
    "        out_values = np.zeros((self.NUMER_OF_RECORDS, self.MAXIMAL_LENGTH+1), dtype=np.int32)\n",
    "\n",
    "        i = 0\n",
    "        for texts, cql in self.data_unprocessed:\n",
    "            for text in texts:\n",
    "                in_values[i, :len(text)] = text\n",
    "                out_values[i, :len(cql)] = cql\n",
    "                in_values[i, len(text)] = EOS_token\n",
    "                out_values[i, len(cql)] = EOS_token\n",
    "                i += 1\n",
    "\n",
    "        self.inputs = torch.LongTensor(in_values).to(device)\n",
    "        self.outputs = torch.LongTensor(out_values).to(device)\n",
    "\n",
    "        for val in self.cql2id:\n",
    "            self.id2cql[self.cql2id[val]] = val\n",
    "\n",
    "    def prepare_for_eval(self, dataset):\n",
    "        max_len = dataset.MAXIMAL_LENGTH\n",
    "\n",
    "        self.inputs = self.inputs.to(\"cpu\")\n",
    "        self.outputs = self.outputs.to(\"cpu\")\n",
    "        in_values = self.inputs.numpy()\n",
    "        out_values = self.outputs.numpy()\n",
    "\n",
    "        in_values_new = np.zeros((self.NUMER_OF_RECORDS, max_len+1), dtype=np.int32)\n",
    "        out_values_new = np.zeros((self.NUMER_OF_RECORDS, max_len+1), dtype=np.int32)\n",
    "\n",
    "        if max_len > self.MAXIMAL_LENGTH:\n",
    "            for i in range(self.NUMER_OF_RECORDS):\n",
    "                for j in range(self.MAXIMAL_LENGTH+1):\n",
    "                    val = in_values[i][j]\n",
    "                    if val >= len(dataset.word2id):\n",
    "                        val = EMP_token\n",
    "                    in_values_new[i][j] = val\n",
    "                    out_values_new[i][j] = out_values[i][j]\n",
    "\n",
    "        else:\n",
    "            for i in range(self.NUMER_OF_RECORDS):\n",
    "                for j in range(max_len):\n",
    "                    val = in_values[i][j]\n",
    "                    if val >= len(dataset.word2id):\n",
    "                        val = EMP_token\n",
    "                    in_values_new[i][j] = val\n",
    "                    out_values_new[i][j] = out_values[i][j]\n",
    "                in_values_new[i][max_len] = EMP_token\n",
    "                out_values_new[i][max_len] = EMP_token\n",
    "\n",
    "        self.inputs = torch.LongTensor(in_values_new).to(device)\n",
    "        self.outputs = torch.LongTensor(out_values_new).to(device)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.NUMER_OF_RECORDS\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.NUMER_OF_RECORDS:\n",
    "            return self.inputs[idx], self.outputs[idx]\n",
    "        idx = idx % len(self.texts)\n",
    "        text_tensor = self.inputs[idx].clone()\n",
    "        text_tensor[random.randint(0, len(self.inputs[idx]) - 1)] = EMP_token\n",
    "        return text_tensor, self.cqls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8975ccdc-111c-4582-ae94-1876427fe15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_natural2cql_train = DatasetNatural2CQL()\n",
    "dataset_natural2cql_train.load_pandas(dataset_pandas_train)\n",
    "dataset_natural2cql_train.finalize(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456c17cd-40f3-445c-9fb4-457031d6b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_natural2cql_validation = DatasetNatural2CQL()\n",
    "dataset_natural2cql_validation.word2id = dataset_natural2cql_train.word2id.copy()\n",
    "dataset_natural2cql_validation.cql2id = dataset_natural2cql_train.cql2id.copy()\n",
    "dataset_natural2cql_validation.load_pandas(dataset_pandas_validate)\n",
    "dataset_natural2cql_validation.finalize(device)\n",
    "dataset_natural2cql_validation.prepare_for_eval(dataset_natural2cql_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4efff303-e721-4013-8046-423842387675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_natural2cql_test = DatasetNatural2CQL()\n",
    "dataset_natural2cql_test.word2id = dataset_natural2cql_train.word2id.copy()\n",
    "dataset_natural2cql_test.cql2id = dataset_natural2cql_train.cql2id.copy()\n",
    "dataset_natural2cql_test.load_pandas(dataset_pandas_test)\n",
    "dataset_natural2cql_test.finalize(device)\n",
    "dataset_natural2cql_test.prepare_for_eval(dataset_natural2cql_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b5bab0-6696-473a-8273-5ebbe28d4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(encoder, decoder, dataset, prefix=\"\"):\n",
    "    torch.save(encoder.state_dict(), prefix + \"encoder.pt\")\n",
    "    torch.save(decoder.state_dict(), prefix + \"decoder.pt\")\n",
    "    with open(prefix + \"metadata.txt\", \"w\") as f:\n",
    "        f.write(f\"{dataset.MAXIMAL_LENGTH}\\n\")\n",
    "        f.write(f\"{json.dumps(dataset.word2id)}\\n\")\n",
    "        f.write(f\"{json.dumps(dataset.id2cql)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4bcbd2-46a2-4ab4-87b2-4c04baabfd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33df3bfe-7485-41ac-b25b-b4db5426f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ed6fa0-c381-43d5-9c09-24f222e99c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "421132c0-c181-47a3-ad74-1d0076cdb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalueate(dataset, encoder, decoder, cql2id):\n",
    "    space_token = cql2id[\" \"]\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "\n",
    "    # exact match\n",
    "    exact_match = 0\n",
    "    tested_match = 0\n",
    "    for data in DataLoader(dataset, batch_size=64, shuffle=False):\n",
    "        input_tensor, target_tensor = data\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "            decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "        target_tensor = target_tensor.to(\"cpu\")\n",
    "        decoded_ids = decoded_ids.to(\"cpu\")\n",
    "        target_list = target_tensor.tolist()\n",
    "        decoded_list = decoded_ids.tolist()\n",
    "        for val in range(len(decoded_list)):\n",
    "            EOS_pos = 0\n",
    "            while EOS_pos < len(decoded_list[val]) and decoded_list[val][EOS_pos] != EOS_token:\n",
    "                EOS_pos += 1\n",
    "            decoded_list[val] = decoded_list[val][:EOS_pos]\n",
    "\n",
    "            EOS_pos = 0\n",
    "            while EOS_pos < len(target_list[val]) and target_list[val][EOS_pos] != EOS_token:\n",
    "                EOS_pos += 1\n",
    "            target_list[val] = target_list[val][:EOS_pos]\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    decoded_list[val].remove(space_token)\n",
    "                except ValueError:\n",
    "                    break\n",
    "            while True:\n",
    "                try:\n",
    "                    target_list[val].remove(space_token)\n",
    "                except ValueError:\n",
    "                    break\n",
    "            if decoded_list[val] == target_list[val]:\n",
    "                exact_match += 1\n",
    "            tested_match += 1\n",
    "    return exact_match, tested_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b09ab35b-55c9-4d1b-a326-abfbd0b45127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, logger_file=None):\n",
    "    total_loss = 0\n",
    "    iterations = 0\n",
    "    start_time = time.time()\n",
    "    for data in dataloader:\n",
    "        if iterations % 100 == 0:\n",
    "            print(f\"Iteration: {iterations}/{len(dataloader)}\")\n",
    "        input_tensor, target_tensor = data\n",
    "        iterations += 1\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "    print(f\"[{datetime.datetime.now().isoformat()}] Epoch took: {int(end_time - start_time)} seconds.\")\n",
    "    save_model(encoder, decoder, dataset_natural2cql_train, \"autosave_\")\n",
    "    \n",
    "    exact_match, num_of_match = evalueate(dataset_natural2cql_validation, encoder, decoder, dataset_natural2cql_train.cql2id)\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "\n",
    "    print(f\"[{datetime.datetime.now().isoformat()}] Match: {exact_match / num_of_match} ({exact_match} / {num_of_match})\\n\")\n",
    "    \n",
    "    if logger_file is not None:\n",
    "        logger_file.write(f\"[{datetime.datetime.now().isoformat()}] Epoch took: {int(end_time - start_time)} seconds.\\n\")\n",
    "        logger_file.write(f\"[{datetime.datetime.now().isoformat()}] Loss: {total_loss / len(dataloader)}\\n\")\n",
    "        logger_file.write(f\"[{datetime.datetime.now().isoformat()}] Match: {exact_match / num_of_match} ({exact_match} / {num_of_match})\\n\")\n",
    "        logger_file.flush()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "491ea9c5-6ec0-4760-8638-a30945920e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001):\n",
    "        encoder.train(True)\n",
    "        decoder.train(True)\n",
    "        logger_file = open(\"log_fix.txt\", \"a\")\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "        criterion = nn.NLLLoss()\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, logger_file=logger_file)\n",
    "            print(f\"[{datetime.datetime.now().isoformat()}] Loss: {loss}\")\n",
    "        logger_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f323135-71f5-4ce3-bc5f-a8f022fd4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "encoder = EncoderRNN(len(dataset_natural2cql_train.word2id), hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(dataset_natural2cql_train.cql2id), dataset_natural2cql_train.MAXIMAL_LENGTH+1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55923e89-e307-4fee-8f8e-3058a0d5ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0/1328\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset_natural2cql_train, batch_size=64, shuffle=True)\n",
    "train(train_dataloader, encoder, decoder, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5305e99-0dee-4d1f-862f-3b2df03a4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(sentence, max_length, word2id):\n",
    "    sent = word_tokenize(sentence)\n",
    "    values = []\n",
    "    for t in sent:\n",
    "        try:\n",
    "            values.append(word2id[t])\n",
    "        except Exception:\n",
    "            values.append(EMP_token)\n",
    "    res = np.zeros(max_length+1, np.int32)\n",
    "    res[:len(values)] = values\n",
    "    res[len(values)] = EOS_token\n",
    "    return np.array([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136b01c-5a3a-4bb8-a74d-442caf4d81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output(sentence, cql2id):\n",
    "    response = []\n",
    "    for t in sentence:\n",
    "        if t == EOS_token:\n",
    "            break\n",
    "        response.append(cql2id[int(t)])\n",
    "    return \"\".join(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a9697-fdee-4774-b654-35c3e5ff8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = make_input(\"All lemmas cat followed by any verb or any noun.\", dataset_natural2cql_train.MAXIMAL_LENGTH, dataset_natural2cql_train.word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2b05b-e7d5-4668-a195-ee7ebbabf6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_outputs, encoder_hidden = encoder(torch.LongTensor(val).to(device))\n",
    "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3f805-6f09-47bd-ba09-7d4b65c0088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, topi = decoder_outputs.topk(1)\n",
    "decoded_ids = topi.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ed343-360e-4460-8812-660eda497337",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_ids = decoded_ids.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa457e2a-77b8-4cb7-8e7c-472741134456",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_output(decoded_ids, dataset_natural2cql_train.id2cql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
