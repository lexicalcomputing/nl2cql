{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eaccc7-b289-43b0-8049-3a8a54e0db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nl2cql - translation of natural language queries to Corpus Query Language\n",
    "# Copyright 2025 Ota MikuÅ¡ek\n",
    "# This program is licensed under GNU Lesser General Public License\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbaab85e-cefb-46a8-bf9e-0a7810ffc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0b86bf-0963-4944-87c5-51add03cac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(21)\n",
    "torch.cuda.manual_seed_all(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c264eac-c63a-4b2b-bba6-ec98b5cf4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395ed951-d6c5-4e7c-8fd7-907f520e0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQL(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: Optional[str] = None) -> None:\n",
    "        self.sentence_freq = []\n",
    "        self.cql2nl = []\n",
    "        self.nl2cql = []\n",
    "        self.natural_language_rulebased = []\n",
    "        self.cql = []\n",
    "        self.natural_language = []\n",
    "\n",
    "        if path is not None:\n",
    "            self.load_tsv(path)\n",
    "\n",
    "    def add_translation(self, freq: int, cql: str, natural_language_rulebased: str, natural_language: List[str]) -> None:\n",
    "        cql_index = len(self.sentence_freq)\n",
    "        self.sentence_freq.append(freq)\n",
    "        self.cql.append(cql)\n",
    "        self.natural_language_rulebased.append(natural_language_rulebased)\n",
    "        self.cql2nl.append([])\n",
    "\n",
    "        for sentence in natural_language:\n",
    "            self.nl2cql.append(cql_index)\n",
    "            self.cql2nl[-1].append(len(self.natural_language))\n",
    "            self.natural_language.append(sentence)\n",
    "\n",
    "    def load_tsv(self, path: str) -> None:\n",
    "        with open(path, \"r\") as file_data:\n",
    "            for line in file_data:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                texts_json = json.loads(line[4])\n",
    "                texts_extracted = texts_json[\"data\"][0][\"content\"][0][\"text\"][\"value\"].split(\"\\n\")\n",
    "                self.add_translation(int(line[0]), line[2], line[3], texts_extracted)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nl2cql)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.nl2cql):\n",
    "            return self.natural_language[idx], self.cql[self.nl2cql[idx]]\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac3fcf7-1e5f-4623-a448-0649a87b5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQLTokenized(DatasetNatural2CQL):\n",
    "    def __init__(self, tokenizer: Any, path: Optional[str] = None) -> None:\n",
    "        super().__init__(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.natural_language_tokenized = []\n",
    "        self.natural_language_mask = []\n",
    "        self.cql_tokenized = []\n",
    "        if len(self) > 0:\n",
    "            self.tokenize()\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        for sentence in self.natural_language:\n",
    "            sentence_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [\"translate: \" + sentence.replace(\"/\", \"//\")],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.natural_language_tokenized.append(sentence_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "            self.natural_language_mask.append(sentence_tokenized.attention_mask.squeeze().to(dtype=torch.long))\n",
    "\n",
    "        for c in self.cql:\n",
    "            c_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [c],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.cql_tokenized.append(c_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "\n",
    "    def apply_padding(self, mx0, mx2) -> None:\n",
    "        for i in range(len(self.natural_language_tokenized)):\n",
    "            tmp = self.natural_language_mask[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_mask[i] = torch.zeros(mx0)\n",
    "            self.natural_language_mask[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_mask[i] = self.natural_language_mask[i].to(dtype=torch.long)\n",
    "\n",
    "            tmp = self.natural_language_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_tokenized[i] = torch.zeros(mx0)\n",
    "            self.natural_language_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_tokenized[i] = self.natural_language_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "        for i in range(len(self.cql_tokenized)):\n",
    "            tmp = self.cql_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.cql_tokenized[i] = torch.zeros(mx2)\n",
    "            self.cql_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.cql_tokenized[i] = self.cql_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.nl2cql):\n",
    "            return self.natural_language_tokenized[idx], self.natural_language_mask[idx], self.cql_tokenized[self.nl2cql[idx]]\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966413af-2754-450c-80c7-9302b46004da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQLTokenizedSplited(DatasetNatural2CQL):\n",
    "    def __init__(self, tokenizer: Any, path: Optional[str] = None) -> None:\n",
    "        super().__init__(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.natural_language_tokenized = []\n",
    "        self.natural_language_mask = []\n",
    "        self.cql_tokenized = []\n",
    "        self.enabled_natural_language = []\n",
    "        if path is not None:\n",
    "            self.tokenize()\n",
    "\n",
    "    def split_on_cql(self, p):\n",
    "        s = self.cql2nl[:]\n",
    "        random.shuffle(s)\n",
    "        sp = int(len(s) * p / 100)\n",
    "        return s[:sp], s[sp:]\n",
    "    \n",
    "    def enable_cql(self, cqls):\n",
    "        self.enabled_natural_language = []\n",
    "        for cql in cqls:\n",
    "            for p in cql:\n",
    "                self.enabled_natural_language.append(p)\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        for sentence in self.natural_language:\n",
    "            sentence_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [\"translate: \" + sentence.replace(\"/\", \"//\")],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.natural_language_tokenized.append(sentence_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "            self.natural_language_mask.append(sentence_tokenized.attention_mask.squeeze().to(dtype=torch.long))\n",
    "\n",
    "        for c in self.cql:\n",
    "            c_tokenized = self.tokenizer.batch_encode_plus(\n",
    "                [c],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.cql_tokenized.append(c_tokenized.input_ids.squeeze().to(dtype=torch.long))\n",
    "\n",
    "    def apply_padding(self, mx0, mx2) -> None:\n",
    "        for i in range(len(self.natural_language_tokenized)):\n",
    "            tmp = self.natural_language_mask[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_mask[i] = torch.zeros(mx0)\n",
    "            self.natural_language_mask[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_mask[i] = self.natural_language_mask[i].to(dtype=torch.long)\n",
    "\n",
    "            tmp = self.natural_language_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.natural_language_tokenized[i] = torch.zeros(mx0)\n",
    "            self.natural_language_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.natural_language_tokenized[i] = self.natural_language_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "        for i in range(len(self.cql_tokenized)):\n",
    "            tmp = self.cql_tokenized[i]\n",
    "            if len(tmp.shape) == 0:\n",
    "                tmp = tmp.unsqueeze(0)\n",
    "            self.cql_tokenized[i] = torch.zeros(mx2)\n",
    "            self.cql_tokenized[i][:tmp.shape[0]] = tmp\n",
    "            self.cql_tokenized[i] = self.cql_tokenized[i].to(dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enabled_natural_language)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.enabled_natural_language):\n",
    "            return self.natural_language_tokenized[self.enabled_natural_language[idx]], self.natural_language_mask[self.enabled_natural_language[idx]], self.cql_tokenized[self.nl2cql[self.enabled_natural_language[idx]]]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3598ddd-dd1f-453c-b446-61508c5e1d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c77a68-3ed0-41ce-afb2-f386a9e87e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/asteria02/thor1/asteria04_mnt_local2/xmikusek/aiproject/.venv/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = DatasetNatural2CQLTokenizedSplited(tokenizer, \"expand_natural_texts_0004.res.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a0dec3-e66a-4321-96e5-62e696e8243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_cqls, other_cqls = dataset_tokenized.split_on_cql(80)\n",
    "#sp = int(len(other_cqls) * 80 / 100)\n",
    "#valid_cqls, test_cqls = other_cqls[:sp], other_cqls[sp:]\n",
    "\n",
    "train_cqls = []\n",
    "with open(\"train_ids.json\", \"r\") as file:\n",
    "    train_cqls = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f64bb8f-5161-43bf-a857-deee2d1caeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized.enable_cql(train_cqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21665ab7-73a0-464d-bb2f-a30a7c4d7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([13959,    10,     3,    31,     9,    77,    31,    17,    31,     6,\n",
       "             3,    31,    29,    32,    31,     6,    42,     3,    31,  2264,\n",
       "            31,  2348,    57,    80,    42,   192,    73,  7576,  3676, 14145,\n",
       "             7,    11,     3,  5490,    16,     3,    31,    29,    32,  8052,\n",
       "            31,     6,     3,    31,    29,    32,  8352,    31,     6,    42,\n",
       "             3,    31,    29,    32,  6965,    31,     5,     1]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor([  784,  6051, 17592,     9,    77,    31,    17,  9175,    29,    32,\n",
       "          9175,  2264,   121,   908,  6306,   908,     2,  4347,   357,     2,\n",
       "          6306,  6051, 17592,    29,    32,  8052,  9175,    29,    32,  8352,\n",
       "          9175,    29,    32,  6965,   121,   908,     1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset_tokenized))\n",
    "dataset_tokenized[len(dataset_tokenized)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f036b7-d9f6-4d91-ba44-cbf768f5204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mx0 = 352 mx2 = 120\n"
     ]
    }
   ],
   "source": [
    "mx0 = 0\n",
    "mx2 = 0\n",
    "for i, rec in enumerate(dataset_tokenized):\n",
    "    if i >= len(dataset_tokenized):\n",
    "        break\n",
    "    if len(rec[0].shape) > 0 and len(rec[0]) > mx0:\n",
    "        mx0 = rec[0].shape[0]\n",
    "    if len(rec[2].shape) > 0 and len(rec[2]) > mx2:\n",
    "        mx2 = rec[2].shape[0]\n",
    "print(\"mx0 =\", mx0, \"mx2 =\", mx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0890c48c-eb44-4674-86f3-9ddc4aec5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized.apply_padding(mx0, mx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3936ff3a-09e0-468c-9227-ae9392795545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([13959,    10,     3,    31,     9,    77,    31,    17,    31,     6,\n",
       "             3,    31,    29,    32,    31,     6,    42,     3,    31,  2264,\n",
       "            31,  2348,    57,    80,    42,   192,    73,  7576,  3676, 14145,\n",
       "             7,    11,     3,  5490,    16,     3,    31,    29,    32,  8052,\n",
       "            31,     6,     3,    31,    29,    32,  8352,    31,     6,    42,\n",
       "             3,    31,    29,    32,  6965,    31,     5,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([  784,  6051, 17592,     9,    77,    31,    17,  9175,    29,    32,\n",
       "          9175,  2264,   121,   908,  6306,   908,     2,  4347,   357,     2,\n",
       "          6306,  6051, 17592,    29,    32,  8052,  9175,    29,    32,  8352,\n",
       "          9175,    29,    32,  6965,   121,   908,     1,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset_tokenized))\n",
    "dataset_tokenized[len(dataset_tokenized)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc2627e-75ef-4cb4-9ed0-95229483f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired from: https://github.com/Shivanandroy/T5-Finetuning-PyTorch\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer, log_file, save_prefix):\n",
    "    epoch_start = datetime.datetime.utcnow()\n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[2].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[0].to(device, dtype=torch.long)\n",
    "        mask = data[1].to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "\n",
    "        time_delta = datetime.datetime.utcnow() - epoch_start\n",
    "        if _ % 100 == 0:\n",
    "            remains_seconds = time_delta.seconds / (_ + 0.01) * (len(loader) - _)\n",
    "            print(\"time: \", datetime.datetime.utcnow().isoformat(), time_delta.seconds , \"sec | ETA:\", \"%.2f\" % (remains_seconds / 3600, ) , \"h | epoch: \", str(epoch), \"| batch: \", str(_), \"/\", len(loader), \"|\", str(loss.item()), file=log_file)\n",
    "            print(\"time: \", datetime.datetime.utcnow().isoformat(), time_delta.seconds , \"sec | ETA:\", \"%.2f\" % (remains_seconds / 3600, ) , \"h | epoch: \", str(epoch), \"| batch: \", str(_), \"/\", len(loader), \"|\", str(loss.item()))\n",
    "            log_file.flush()\n",
    "\n",
    "        if _ % 1000 == 0:\n",
    "            torch.save(model.state_dict(), save_prefix + str(epoch) + \"_\" + str(_) + \".pt\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(model.state_dict(), save_prefix + str(epoch) + \"_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cfa7f91-4da5-4bdc-9f12-4fd1380b20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307bb6a-22c2-4150-b026-271e608ab601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  2025-04-01T21:31:55.792818 1 sec | ETA: 168.64 h | epoch:  0 | batch:  0 / 6071 | 8.359023094177246\n"
     ]
    }
   ],
   "source": [
    "with open(\"playground_log\", \"a\") as log_file:\n",
    "    for i in range(200):\n",
    "        train(i, tokenizer, model, device, torch.utils.data.DataLoader(dataset_tokenized, batch_size=14, shuffle=True, num_workers=0), optimizer, log_file, \"models/google-t5_t5-base/model_train_data_\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d65edb8d-89a2-4959-91e1-0c77cda72b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,    10,  2372,  1448,  2348,    57,    90,   635,     9,  1365,\n",
      "            38,  7860,   150,   202,    11,   258,  1684,    28,   539,   441,\n",
      "          7142,     5,     1]])\n",
      "tensor([[    0,     3, 24519, 18030, 13407,    20,    90,   635,     9,  1365,\n",
      "           212,  7375,  7375,   267,  4809,     6,   111, 18072,   123,   539,\n",
      "           441]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "sentence_tokenized = tokenizer(\n",
    "    \"translate: Any word followed by lemma door as tag noun and then starting with open within sentence.\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(sentence_tokenized.input_ids)\n",
    "generated_ids = model.generate(\n",
    "      sentence_tokenized.input_ids.to(\"cuda\")\n",
    ")\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e82a70-d44b-4737-8768-c1eccbe462df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(generated_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
