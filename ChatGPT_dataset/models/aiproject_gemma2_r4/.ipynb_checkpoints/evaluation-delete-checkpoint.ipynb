{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf874df-65bb-4853-9769-c2a56dec9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import datasets\n",
    "import time\n",
    "\n",
    "import json\n",
    "import os\n",
    "import cqlcmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fccd5b14-0616-4d80-9536-d6e8430e946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(21)\n",
    "torch.cuda.manual_seed_all(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3527179-090d-48ae-bdb5-f4eef462cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc981f2-f58d-4be8-88f6-955d973455cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNatural2CQL(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: Optional[str] = None) -> None:\n",
    "        self.sentence_freq = []\n",
    "        self.cql2nl = []\n",
    "        self.nl2cql = []\n",
    "        self.natural_language_rulebased = []\n",
    "        self.cql = []\n",
    "        self.natural_language = []\n",
    "        self.enabled_natural_language = []\n",
    "\n",
    "        if path is not None:\n",
    "            self.load_tsv(path)\n",
    "\n",
    "    def enable_cql(self, cqls):\n",
    "        self.enabled_natural_language = []\n",
    "        for cql in cqls:\n",
    "            for p in cql:\n",
    "                self.enabled_natural_language.append(p)\n",
    "\n",
    "    def dump_json(self, filepath: str) -> None:\n",
    "        with open(filepath, \"w\") as file:\n",
    "            for i in range(len(self)):\n",
    "                data = json.dumps(self[i])\n",
    "                file.write(data)\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "    def add_translation(self, freq: int, cql: str, natural_language_rulebased: str, natural_language: List[str]) -> None:\n",
    "        cql_index = len(self.sentence_freq)\n",
    "        self.sentence_freq.append(freq)\n",
    "        self.cql.append(cql)\n",
    "        self.natural_language_rulebased.append(natural_language_rulebased)\n",
    "        self.cql2nl.append([])\n",
    "\n",
    "        for sentence in natural_language:\n",
    "            self.nl2cql.append(cql_index)\n",
    "            self.cql2nl[-1].append(len(self.natural_language))\n",
    "            self.natural_language.append(sentence)\n",
    "\n",
    "    def load_tsv(self, path: str) -> None:\n",
    "        with open(path, \"r\") as file_data:\n",
    "            for line in file_data:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                texts_json = json.loads(line[4])\n",
    "                texts_extracted = texts_json[\"data\"][0][\"content\"][0][\"text\"][\"value\"].split(\"\\n\")\n",
    "                self.add_translation(int(line[0]), line[2], line[3], texts_extracted)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enabled_natural_language)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.nl2cql):\n",
    "            return {\"text\": self.natural_language[self.enabled_natural_language[idx]], \"cql\": self.cql[self.nl2cql[self.enabled_natural_language[idx]]]}\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e02dab6-961b-48c7-bf75-ef953c5c9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetNatural2CQL(\"expand_natural_texts_0004.res.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfbff40f-252f-483e-8572-225ee838bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cqls = []\n",
    "with open(\"train_ids.json\", \"r\") as file:\n",
    "    validation_cqls = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c0898f-9db3-4673-b22c-9de62a79e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.enable_cql(validation_cqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4df87f9-d445-49a8-bf6b-52f1be2da4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9dfd4692e84d069c07181f06d39c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model_name = \"google/gemma-2-2b\"\n",
    "adapter_model_name = \"outputs/checkpoint-102000\"\n",
    "\n",
    "model_orig = AutoModelForCausalLM.from_pretrained(base_model_name, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model_orig, adapter_model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed42c57a-b4aa-49bf-9990-e06d872b1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'outputs'\n",
    "\n",
    "# List all folders (directories) in the given path\n",
    "folders = [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab5f849b-cfe3-40f6-848a-809077ba6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_in(data):\n",
    "    full_input = \"\"\"Translate Natural Language into CQL Queries like:\n",
    "\n",
    "Word dog followed by lemma run, and then followed by a noun. CQL:\n",
    "```\n",
    "[word=\"dog\"][lemma=\"run\"][pos=\"NN\"]\n",
    "```\n",
    "\n",
    "Lemma \"be\" optionally followed by the word \"not\". CQL:\n",
    "```\n",
    "[lemma=\"be\"][word=\"not\"]? \n",
    "```\n",
    "\n",
    "\"\"\" + data + \" CQL:\\n```\\n\"\n",
    "    return full_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ab3133-91ec-4dd6-a384-fbc438460706",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset.enabled_natural_language)\n",
    "\n",
    "i = 0\n",
    "while i < len(dataset):\n",
    "    dataset.natural_language[dataset.enabled_natural_language[i]] = f_in(dataset.natural_language[dataset.enabled_natural_language[i]])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c394b223-ded0-464d-aaaf-bfa1ed229cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec4287e3-b84a-465c-bedb-56b9201f27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cql(text):\n",
    "    text = text.split(\"<eos>\")[0]\n",
    "    text = text.split(\"CQL:\")[3]\n",
    "    text = text.split(\"```\")[1]\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea26fc-d306-49ab-92df-d2ef783fc5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/asteria02/thor1/asteria04_mnt_local2/xmikusek/aiproject/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"validation_log.txt\", \"w\") as log:\n",
    "    for folder in folders:\n",
    "        start_time = int(time.time())\n",
    "        adapter_model_name = \"outputs/\" + folder\n",
    "\n",
    "        if folder not in [\"checkpoint-20000\", \"checkpoint-40000\", \"checkpoint-60000\", \"checkpoint-80000\", \"checkpoint-100000\", \"checkpoint-122000\", \"checkpoint-142000\", \"checkpoint-162000\", \"checkpoint-182000\", \"checkpoint-192000\"]:\n",
    "            continue\n",
    "        \n",
    "        model = PeftModel.from_pretrained(model_orig, adapter_model_name)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "        blue_sum = 0\n",
    "        blue_size = 0\n",
    "        batch_id = 0\n",
    "        for batch in dataloader:\n",
    "            batch_id += 1\n",
    "            inputs = tokenizer(batch[\"text\"], truncation=True, max_length=1024, return_tensors=\"pt\", padding=True).to(device)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "            for i, output in enumerate(outputs):\n",
    "                cql = get_cql(tokenizer.decode(output, skip_special_tokens=True))\n",
    "                cql_ids = cqlcmp.cql_tokenizer(cql)\n",
    "                gold_cql_ids = cqlcmp.cql_tokenizer(batch[\"cql\"][i])\n",
    "                bleu = cqlcmp.sentence_bleu([gold_cql_ids], cql_ids, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "                blue_sum += bleu\n",
    "                blue_size += 1\n",
    "            to_log = \"\"\n",
    "            to_log += \"Working on: \" + adapter_model_name + \" | \"\n",
    "            to_log += str(blue_size) + \"/\" + str(len(dataset)) + \" | \"\n",
    "            to_log += \"AVG Bleu: \" + str(blue_sum/blue_size) + \" | \"\n",
    "            to_log += \"Time: \" + str(int(time.time())-start_time) + \" | \"\n",
    "            to_log += \"ETA: \" + str(int((int(time.time())-start_time)/(blue_size/len(dataset)*(1-(blue_size/len(dataset)))))) + \" sec\" + \" | \"\n",
    "            \n",
    "            print(to_log, file=log)\n",
    "            log.flush()\n",
    "            print(to_log)\n",
    "            if batch_id >= 15:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a46976-7108-4f03-90c9-aa1312fa7dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
